# FR

Le projet *"Searching"* est un projet qui nait dans le cadre d'une pratique artistique. Dans le cadre de ce projet j'utilise un rasberry pi un pi camera et des mod√®les tensorflow, leur utilisation est document√© ici. De par son cadre artistique la documentation est orient√© vers une utilisation sp√©cifique de ces outils.

Searching consiste en la production d'une s√©rie d'√©pisodes de court film. Ces films sont r√©alis√©es par des personnes diff√©rentes mais ont comme point commun l'utilisation d'une cam√©ra dot√© de reconnaissance de patterns.


---


## Mat√©riel Utilis√©

- **Raspberry Pi 4** : Flasch√© en 32 bits. Le choix du 32 bits est li√© √† des contraintes d‚Äôutilisation de la librairie PiCamera et OpenCV. En effet, MMAL (Multi-Media Abstraction Layer) n'est pas pris en charge en 64 bits. Pour plus d'informations, vous pouvez consulter ce post du forum Raspberry Pi concernant l'erreur [MMAL 64-bit support](https://github.com/raspberrypi/userland/issues/688).
  
- **PiCamera V2** : R√©solution vid√©o de 1080p √† 47 fps, 1640 x 1232 √† 41 fps et 640 x 480 √† 206 fps, ce qui est important pour le workflow de capture vid√©o. [Sp√©cifications mat√©rielles](https://www.raspberrypi.com/documentation/accessories/camera.html).

- **Google Coral USB Accelerator** : Utilis√© pour acc√©l√©rer les mod√®les TensorFlow Lite via le Edge TPU (Tensor Processing Unit).

### Installation du Raspberry Pi OS (32-bit)

1. T√©l√©chargez l‚Äôimage de Raspberry Pi OS (version 32-bit) et flashez l‚Äôimage sur une carte SD en utilisant un outil comme [Raspberry Pi Imager](https://www.raspberrypi.org/software/).
2. Ins√©rez la carte SD dans le Raspberry Pi et d√©marrez le syst√®me.

Installation de Pycoral avec USB Accelerator

- [Get started with the USB Accelerator](https://coral.ai/docs/accelerator/get-started). Possibiliter d'instaler aussi via la commande:

```
pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral~=2.0
```

Si sudo apt-get install python3-pycoral ne fonction pas 

### Creation du mod√®le:

## Entra√Ænement du Mod√®le

J'ai utilis√© la documentation officielle de Coral pour cr√©er un mod√®le de d√©tection d'objets optimis√© pour le Edge TPU. Voici les √©tapes :

1. Suivez le tutoriel [Retrain SSD MobileNet V1 Object Detector on Google Colab (TF1)](https://coral.ai/docs/edgetpu/retrain-detection/) pour cr√©er votre mod√®le.


**Command to Create a TFRecord**:

If you need to create a TFRecord, use the following command:

    python3 object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=/tensorflow/models/research/learn_pet/pet/pet_label_map.pbtxt --data_dir=/tensorflow/models/research/learn_pet/pet/ --output_dir=/tensorflow/models/research/learn_pet//pet/

  
2. **Remarque importante** : Le processus d'entra√Ænement n√©cessite Docker configur√© pour l'architecture **AMD64**, ce qui n'est pas compatible directement avec un Raspberry Pi (qui utilise l'architecture **armv7i**). J'ai donc install√© Docker sur Windows et utilis√© un √©mulateur Linux (comme **WSL2**) pour r√©aliser l'entra√Ænement.

3. Pour plus d'informations sur l'utilisation de Docker et la cr√©ation de mod√®les, je recommande cette vid√©o de [Edgecate:**DIY Custom Object Detection Model via Transfer Learning (TensorFlow Lite Edge TPU)](https://www.youtube.com/watch?v=OJ6IXygqgME&t=850s)**.

4. **Alternative avec Google Colab** : Vous pouvez √©galement utiliser Google Colab pour entra√Æner votre mod√®le en utilisant ce [notebook Google Colab](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_ssdlite_mobiledet_qat_tf1.ipynb#scrollTo=jcApdURAK28f).

### Utilisation du mod√®le

1. Afin de r√©aliser mes vid√©os en meilleure r√©solution et avec un bon taux de FPS, j'utilise deux scripts.
- Le premier fait un enregistrement en direct de la PiCamera avec OpenCV en 1280x720 √† 16 FPS, tout en effectuant simultan√©ment une visualisation de la d√©tection en 640x420. Toutefois, cette derni√®re ne contient pas les bo√Ætes de d√©tection sur la vid√©o enregistr√©e en 1280x720, ce qui permet de gagner en FPS. Ce script g√©n√®re √©galement un tableau CSV qui sera ensuite utilis√© pour la g√©n√©ration du son de la vid√©o. Il sert aussi √† nommer les fichiers en fonction de la derni√®re frame enregistr√©e avant. Cela 
permet de cr√©er une s√©quence de capture vid√©o.
  - Ce premier script est inspir√© d'une version modifi√©e du script [TFLite_detection_webcam.py](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) du d√©p√¥t **TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi** d'EdjeElectronics.
tps://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) de EdjeElectronics
- Le deuxi√®me script compiles les vid√©os √† l'aide de ffmpeg et les analysent en dessinant les boites englobantes.


## Ressources Utiles

Voici une liste de ressources qui peuvent vous aider tout au long du projet.

### Tutoriels pour la cr√©ation de mod√®les TFLite

- [Retrain SSD MobileNet V1 Object Detector on Google Colab (TF1)](https://coral.ai/docs/edgetpu/retrain-detection/) ‚Äì Tutoriel officiel de Coral pour entra√Æner un mod√®le de d√©tection d'objets sur Google Colab.
- [DIY Custom Object Detection Model via Transfer Learning (TensorFlow Lite Edge TPU)](https://www.youtube.com/watch?v=OJ6IXygqgME&t=850s) ‚Äì Tutoriel vid√©o de [Edgecate](https://www.youtube.com/@edgecate) expliquant comment cr√©er un mod√®le de d√©tection d'objets personnalis√© √† l'aide de TensorFlow Lite.
- [Notebook Google Colab pour l'entra√Ænement de mod√®les SSD MobileNet](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_ssdlite_mobiledet_qat_tf1.ipynb#scrollTo=jcApdURAK28f) ‚Äì Utilisez ce notebook pour entra√Æner des mod√®les directement sur Google Colab.

### Ressources suppl√©mentaires pour la conversion des mod√®les pour Edge TPU

- [Google Coral GitHub](https://github.com/google-coral/examples-camera) ‚Äì D√©p√¥t GitHub avec des exemples d'utilisation de Coral et des Raspberry Pi Camera.
- [Edge TPU Compiler Documentation](https://coral.ai/docs/edgetpu/compiler/) ‚Äì Documentation officielle pour compiler un mod√®le TensorFlow Lite pour Edge TPU.

### D√©p√¥ts GitHub utiles

- [Google Coral Edge TPU Examples](https://github.com/google-coral/examples-camera) ‚Äì Exemples d'utilisation du Google Coral USB Accelerator avec des Raspberry Pi et des cam√©ras.

### Documentation TensorFlow

- [Documentation officielle de TensorFlow Lite](https://www.tensorflow.org/lite) ‚Äì Guide complet sur TensorFlow Lite, y compris la conversion et l'optimisation des mod√®les.

### Utilisation de Yolo

L'utilisation d'un mod√®le Yolo est aussi une solution, l'ayant envisag√© un moment voici les d√©p√¥ts et docs que j'ai trouv√© pour utiliser ces mod√®les.
- [YOLOv5 Conversion Guide](https://docs.ultralytics.com/fr/modes/export/) ‚Äì Tutoriel pour la conversion des mod√®les YOLOv5 en mod√®les TensorFlow Lite optimis√©s pour le Edge TPU.
- [edge-tpu-silva GitHub Repository](https://github.com/DAVIDNYARKO123/edge-tpu-silva) ‚Äì Un autre d√©p√¥t pour des exemples de projets utilisant le Coral USB Accelerator.

- [Ultralytics Documentation](https://docs.ultralytics.com/fr/modes/export/)

- [Coral Edge TPU on Raspberry Pi with Ultralytics YOLO11 üöÄ](https://docs.ultralytics.com/fr/guides/coral-edge-tpu-on-raspberry-pi/)

## Conversion du mod√®le YOLO

Si vous souhaitez convertir un mod√®le YOLO en un mod√®le `.tflite` compatible avec EdgeTPU, vous devez appliquer la **quantification int8**. Voici ce que j'ai appris jusqu'√† pr√©sent :

- J'ai essay√© de convertir un mod√®le YOLOv8 en un mod√®le `.tflite` compatible avec EdgeTPU pour l'utiliser avec l'interpr√©teur TensorFlow Lite optimis√© pour Coral. Bien que j'aie rencontr√© quelques probl√®mes, je pense qu'il est possible de le faire fonctionner.
- Vous pouvez consulter cette vid√©o sur [Coral TPU YOLOv5s](https://www.youtube.com/watch?v=D9IExho8pwo) pour la d√©tection d'objets en direct utilisant un mod√®le YOLO sur EdgeTPU.

Pour convertir un mod√®le YOLOv5, le d√©p√¥t GitHub suivant peut √™tre utile :

- [D√©p√¥t de conversion YOLOv5](https://github.com/zldrobit/yolov5)


## Probl√®mes et D√©pannage

### Probl√®me de compatibilit√© avec Docker et Raspberry Pi

Lors de la cr√©ation du mod√®le avec Docker, j'ai rencontr√© des probl√®mes de compatibilit√© li√©s √† l'architecture. Docker sur Raspberry Pi utilise **armv7i**, tandis que certains outils de cr√©ation de mod√®les (comme ceux utilis√©s pour l'Edge TPU) n√©cessitent une architecture **AMD64**. Pour contourner cette limitation, j'ai utilis√© Docker sur un syst√®me Windows via WSL2 (Windows Subsystem for Linux).

### Probl√®me de compatibilit√© MMAL en 64-bit

Le probl√®me d'incompatibilit√© avec MMAL (Multi-Media Abstraction Layer) en 64 bits emp√™che l'utilisation de certaines fonctionnalit√©s de la cam√©ra Pi sur des syst√®mes 64-bit. Le Raspberry Pi OS 32-bit est n√©cessaire pour garantir la compatibilit√© avec la cam√©ra Pi et les biblioth√®ques comme OpenCV et PiCamera. Vous pouvez consulter plus de d√©tails dans le post du forum Raspberry Pi [MMAL 64-bit support](https://github.com/raspberrypi/userland/issues/688).

### Probl√®me de compatibilit√© avec picamera module 3 et Raspberry Pi 5 et python 3.9

la lib picamera n'est pas possible √† installer dans un environement virtuel python 3.9 

```
https://github.com/raspberrypi/picamera2/issues/446
https://github.com/raspberrypi/picamera2/issues/503
```

Or, il faut un environnement virtuel pour installer **PyCoral** qui fonctionne sur Python 3.9.  
**Picamera2** n√©cessite **libcamera**, qui ne peut √™tre install√© que via **apt** (`sudo apt install`). Cela emp√™che son installation dans un environnement virtuel Conda.  
**JungLearnBot** propose un d√©tournement possible sur Python 3.11.  
- [Readme.RPi5.cpu.picam.qt.md](https://github.com/JungLearnBot/RPi5_yolov8/blob/main/Readme.RPi5.cpu.picam.qt.md)

### Probl√®me de compatibilit√© entre OpenCV et libcamera (Picamera2)

**OpenCV VideoCapture** n'est pas compatible avec **Pi Camera Module 3**. Cela est d√ª √† la fa√ßon dont **libcamera** g√®re les flux vid√©o, ce qui pose des probl√®mes d'acc√®s direct √† la cam√©ra avec OpenCV.  
M√™me apr√®s l'installation de **Picamera2**, la fonction **cv2.imshow()** de **OpenCV** ne fonctionne pas correctement. L'application se fige lorsque tu essaies d'afficher le flux de la cam√©ra.  
En r√©ponse, **JungLearnBot** propose d'opter pour **Qt** pour la visualisation des flux vid√©o, mais il y a aussi des probl√®mes de compatibilit√© avec OpenCV sous **Pi OS** (ce qui n√©cessite l'installation de **opencv-python-headless** pour √©viter les conflits).

Utiliser **Picamera2** √† la place de **OpenCV VideoCapture**, mais cela n√©cessite de g√©rer les installations de **libcamera** et de manipuler les biblioth√®ques globales via des d√©tournements possibles sur Python 3.11.

Sur **Pi Camera Module 3**, utiliser **Qt** pour la visualisation vid√©o, car **cv2.imshow** pose des probl√®mes avec **Picamera2** et installer **opencv-python-headless** pour √©viter les conflits avec **Qt** sur **Pi OS**.


---


## Ressources utiles

### [DAVID NYARKO](https://github.com/DAVIDNYARKO123)
- [D√©p√¥t GitHub edge-tpu-silva](https://github.com/DAVIDNYARKO123/edge-tpu-silva)

- [Edje Electronics / TensorFlow Lite pour la d√©tection d'objets sur Android et Raspberry Pi](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/tree/master)

### [Documentation Ultralytics](https://docs.ultralytics.com/fr/modes/export/)

- [Coral Edge TPU sur Raspberry Pi avec Ultralytics YOLO11 üöÄ](https://docs.ultralytics.com/fr/guides/coral-edge-tpu-on-raspberry-pi/)

### Ressources Coral

- [Commencer avec l'acc√©l√©rateur USB](https://coral.ai/docs/accelerator/get-started)
  
#### Ressources Coral suppl√©mentaires :

- [Google-Coral GitHub](https://github.com/google-coral/examples-camera)
    - [Exemples de cam√©ra Raspberry Pi](https://github.com/google-coral/examples-camera/tree/master/raspicam)
  
- [Documentation du compilateur Edge TPU](https://coral.ai/docs/edgetpu/compiler/)

### Pour PI5 and picamera3: [JungLearnBot](https://github.com/JungLearnBot)
- [RPi5_yolov8](https://github.com/JungLearnBot/RPi5_yolov8)


### Cr√©ation de la coque: [Kevin McAleer](https://www.youtube.com/@kevinmcaleer28)
- [Is the Raspberry Pi High-Quality Camera worth it? & Build your own Camera
](https://www.youtube.com/watch?v=4BEjKUK8DSQ)

---


ENG


# TensorFlow - Model Conversion and Edge TPU 

## Create Your Own EdgeTPU-Compatible `.tflite` Model

**Creating a TensorFlow Lite Model**:  
   I used the tutorial [Retrain SSD MobileNet V1 Object Detector on Google Colab (TF1)](https://coral.ai/docs/edgetpu/retrain-detection/) to create my EdgeTPU-compatible model. I faced issues with two Google Colab tutorials, but this one worked well for me.
   
**Video Tutorials**:
    - [DIY Custom Object Detection Model via Transfer Learning (TensorFlow Lite Edge TPU)](https://www.youtube.com/watch?v=OJ6IXygqgME&t=217s) by [Edgecate](https://www.youtube.com/@edgecate)
    
**Command to Create a TFRecord**:

If you need to create a TFRecord, use the following command:

    python3 object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=/tensorflow/models/research/learn_pet/pet/pet_label_map.pbtxt --data_dir=/tensorflow/models/research/learn_pet/pet/ --output_dir=/tensorflow/models/research/learn_pet//pet/

After model train convert it for Edge TPU

   
    ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num 500
	

4. **Inspecting Your Model**:
    - You can use [Netron](https://netron.app/) to check the structure of your model.

5. **Important Notes**:
    - Be cautious: for Docker, you need an AMD64 architecture. **aarch64** and **armv7i** architectures cannot build the Docker image provided by google-coral.
    - If you use a Google Coral, plug the coral on the blue usb port.


## YOLO Model Conversion

If you want to convert a YOLO model to an EdgeTPU-compatible `.tflite` model, you need to apply **int8 quantization**. Here's what I've learned so far:

- I tried converting a YOLOv8 model to an EdgeTPU-compatible `.tflite` model for use with the TensorFlow Lite interpreter optimized for Coral. While I encountered some issues, I believe it's possible to make it work.  
- You can check out this video on [Coral TPU YOLOv5s](https://www.youtube.com/watch?v=D9IExho8pwo) for live object detection using a YOLO model on EdgeTPU.
  
For converting a YOLOv5 model, the following GitHub repository may help:

- [YOLOv5 Conversion Repository](https://github.com/zldrobit/yolov5)
